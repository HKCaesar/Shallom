{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_classes import Data\n",
    "import util as ut\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten,Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "random_state = 1\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='WN18RR'\n",
    "kg_root = 'KGs/'+dataset+'/'\n",
    "dataset=Data(kg_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "x=[]\n",
    "\n",
    "entitiy_idx=dict()\n",
    "\n",
    "sub_obj_pairs= dataset.get_entity_pairs_with_predicates(dataset.train_data)\n",
    "for s_o_pair, predicates in sub_obj_pairs.items():\n",
    "    s,o=s_o_pair\n",
    "    entitiy_idx.setdefault(s, len(entitiy_idx))\n",
    "    entitiy_idx.setdefault(o, len(entitiy_idx))\n",
    "    x.append([entitiy_idx[s],entitiy_idx[o]])\n",
    "    y.append(list(predicates))\n",
    "x=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "['_hypernym']\n"
     ]
    }
   ],
   "source": [
    "print(x[0]) # input\n",
    "print(y[0]) # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "x=np.array(x)\n",
    "binarizer=MultiLabelBinarizer()\n",
    "y=binarizer.fit_transform(y)\n",
    "print(x[0]) # input\n",
    "print(y[0]) # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['_also_see', '_derivationally_related_form', '_has_part',\n",
       "       '_hypernym', '_instance_hypernym', '_member_meronym',\n",
       "       '_member_of_domain_region', '_member_of_domain_usage',\n",
       "       '_similar_to', '_synset_domain_topic_of', '_verb_group'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2, 100)            4055900   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                3311      \n",
      "=================================================================\n",
      "Total params: 4,119,511\n",
      "Trainable params: 4,119,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(entitiy_idx), embedding_dim, input_length=2,activity_regularizer=l2(0.1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(embedding_dim*3, activation='relu',activity_regularizer=l2(0.1)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',  # adam\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86726 samples\n",
      "Epoch 1/100\n",
      "86726/86726 [==============================] - 6s 75us/sample - loss: 1.8792 - accuracy: 0.4132\n",
      "Epoch 2/100\n",
      "86726/86726 [==============================] - 6s 70us/sample - loss: 1.1737 - accuracy: 0.6340\n",
      "Epoch 3/100\n",
      "86726/86726 [==============================] - 7s 77us/sample - loss: 0.7872 - accuracy: 0.8217\n",
      "Epoch 4/100\n",
      "86726/86726 [==============================] - 6s 69us/sample - loss: 0.5990 - accuracy: 0.8705\n",
      "Epoch 5/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.4938 - accuracy: 0.8990\n",
      "Epoch 6/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.4250 - accuracy: 0.9182\n",
      "Epoch 7/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.3760 - accuracy: 0.9321\n",
      "Epoch 8/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.3383 - accuracy: 0.9412\n",
      "Epoch 9/100\n",
      "86726/86726 [==============================] - 6s 67us/sample - loss: 0.3104 - accuracy: 0.9466\n",
      "Epoch 10/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.2862 - accuracy: 0.9525\n",
      "Epoch 11/100\n",
      "86726/86726 [==============================] - 6s 67us/sample - loss: 0.2695 - accuracy: 0.9553\n",
      "Epoch 12/100\n",
      "86726/86726 [==============================] - 6s 66us/sample - loss: 0.2537 - accuracy: 0.9593\n",
      "Epoch 13/100\n",
      "86726/86726 [==============================] - 6s 72us/sample - loss: 0.2395 - accuracy: 0.9624\n",
      "Epoch 14/100\n",
      "86726/86726 [==============================] - 6s 71us/sample - loss: 0.2269 - accuracy: 0.9659\n",
      "Epoch 15/100\n",
      "86726/86726 [==============================] - 6s 69us/sample - loss: 0.2141 - accuracy: 0.9700\n",
      "Epoch 16/100\n",
      "86726/86726 [==============================] - 6s 67us/sample - loss: 0.2029 - accuracy: 0.9730\n",
      "Epoch 17/100\n",
      "86726/86726 [==============================] - 6s 66us/sample - loss: 0.1908 - accuracy: 0.9769\n",
      "Epoch 18/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.1830 - accuracy: 0.9792\n",
      "Epoch 19/100\n",
      "86726/86726 [==============================] - 5s 63us/sample - loss: 0.1728 - accuracy: 0.9817\n",
      "Epoch 20/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.1654 - accuracy: 0.9839\n",
      "Epoch 21/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.1599 - accuracy: 0.9848\n",
      "Epoch 22/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.1525 - accuracy: 0.9856\n",
      "Epoch 23/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.1465 - accuracy: 0.9873\n",
      "Epoch 24/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.1428 - accuracy: 0.9874\n",
      "Epoch 25/100\n",
      "86726/86726 [==============================] - 6s 67us/sample - loss: 0.1364 - accuracy: 0.9891\n",
      "Epoch 26/100\n",
      "86726/86726 [==============================] - 6s 66us/sample - loss: 0.1332 - accuracy: 0.9891\n",
      "Epoch 27/100\n",
      "86726/86726 [==============================] - 7s 78us/sample - loss: 0.1270 - accuracy: 0.9906\n",
      "Epoch 28/100\n",
      "86726/86726 [==============================] - 7s 81us/sample - loss: 0.1250 - accuracy: 0.9899\n",
      "Epoch 29/100\n",
      "86726/86726 [==============================] - 6s 67us/sample - loss: 0.1215 - accuracy: 0.9908\n",
      "Epoch 30/100\n",
      "86726/86726 [==============================] - 6s 69us/sample - loss: 0.1182 - accuracy: 0.9911\n",
      "Epoch 31/100\n",
      "86726/86726 [==============================] - 6s 74us/sample - loss: 0.1158 - accuracy: 0.9912\n",
      "Epoch 32/100\n",
      "86726/86726 [==============================] - 6s 71us/sample - loss: 0.1131 - accuracy: 0.9912\n",
      "Epoch 33/100\n",
      "86726/86726 [==============================] - 9s 102us/sample - loss: 0.1105 - accuracy: 0.9925\n",
      "Epoch 34/100\n",
      "86726/86726 [==============================] - 7s 82us/sample - loss: 0.1094 - accuracy: 0.9915\n",
      "Epoch 35/100\n",
      "86726/86726 [==============================] - 6s 74us/sample - loss: 0.1074 - accuracy: 0.9916\n",
      "Epoch 36/100\n",
      "86726/86726 [==============================] - 7s 79us/sample - loss: 0.1052 - accuracy: 0.9919\n",
      "Epoch 37/100\n",
      "86726/86726 [==============================] - 6s 66us/sample - loss: 0.1027 - accuracy: 0.9925\n",
      "Epoch 38/100\n",
      "86726/86726 [==============================] - 6s 70us/sample - loss: 0.0998 - accuracy: 0.9929\n",
      "Epoch 39/100\n",
      "86726/86726 [==============================] - 7s 83us/sample - loss: 0.0985 - accuracy: 0.9928\n",
      "Epoch 40/100\n",
      "86726/86726 [==============================] - 6s 69us/sample - loss: 0.0975 - accuracy: 0.9925s - loss: 0.0970 - accura\n",
      "Epoch 41/100\n",
      "86726/86726 [==============================] - 6s 73us/sample - loss: 0.0972 - accuracy: 0.9926\n",
      "Epoch 42/100\n",
      "86726/86726 [==============================] - 7s 80us/sample - loss: 0.0953 - accuracy: 0.9927\n",
      "Epoch 43/100\n",
      "86726/86726 [==============================] - 7s 78us/sample - loss: 0.0952 - accuracy: 0.9924\n",
      "Epoch 44/100\n",
      "86726/86726 [==============================] - 7s 85us/sample - loss: 0.0931 - accuracy: 0.9929\n",
      "Epoch 45/100\n",
      "86726/86726 [==============================] - 7s 83us/sample - loss: 0.0917 - accuracy: 0.9927\n",
      "Epoch 46/100\n",
      "86726/86726 [==============================] - 7s 83us/sample - loss: 0.0901 - accuracy: 0.9935\n",
      "Epoch 47/100\n",
      "86726/86726 [==============================] - 6s 74us/sample - loss: 0.0905 - accuracy: 0.9925\n",
      "Epoch 48/100\n",
      "86726/86726 [==============================] - 6s 71us/sample - loss: 0.0884 - accuracy: 0.9929\n",
      "Epoch 49/100\n",
      "86726/86726 [==============================] - 6s 68us/sample - loss: 0.0885 - accuracy: 0.9926\n",
      "Epoch 50/100\n",
      "86726/86726 [==============================] - 6s 66us/sample - loss: 0.0869 - accuracy: 0.9931\n",
      "Epoch 51/100\n",
      "86726/86726 [==============================] - 7s 76us/sample - loss: 0.0867 - accuracy: 0.9931\n",
      "Epoch 52/100\n",
      "86726/86726 [==============================] - 7s 75us/sample - loss: 0.0866 - accuracy: 0.9933\n",
      "Epoch 53/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.0848 - accuracy: 0.9929\n",
      "Epoch 54/100\n",
      "86726/86726 [==============================] - 6s 70us/sample - loss: 0.0836 - accuracy: 0.9934\n",
      "Epoch 55/100\n",
      "86726/86726 [==============================] - 6s 73us/sample - loss: 0.0822 - accuracy: 0.9932\n",
      "Epoch 56/100\n",
      "86726/86726 [==============================] - 6s 68us/sample - loss: 0.0835 - accuracy: 0.9926\n",
      "Epoch 57/100\n",
      "86726/86726 [==============================] - 7s 76us/sample - loss: 0.0837 - accuracy: 0.9927\n",
      "Epoch 58/100\n",
      "86726/86726 [==============================] - 7s 76us/sample - loss: 0.0829 - accuracy: 0.9927\n",
      "Epoch 59/100\n",
      "86726/86726 [==============================] - 6s 69us/sample - loss: 0.0813 - accuracy: 0.9932\n",
      "Epoch 60/100\n",
      "86726/86726 [==============================] - 6s 67us/sample - loss: 0.0812 - accuracy: 0.9933\n",
      "Epoch 61/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.0805 - accuracy: 0.9934\n",
      "Epoch 62/100\n",
      "86726/86726 [==============================] - 6s 68us/sample - loss: 0.0793 - accuracy: 0.9933\n",
      "Epoch 63/100\n",
      "86726/86726 [==============================] - 6s 69us/sample - loss: 0.0789 - accuracy: 0.9934\n",
      "Epoch 64/100\n",
      "86726/86726 [==============================] - 7s 77us/sample - loss: 0.0802 - accuracy: 0.9928\n",
      "Epoch 65/100\n",
      "86726/86726 [==============================] - 7s 77us/sample - loss: 0.0782 - accuracy: 0.9934\n",
      "Epoch 66/100\n",
      "86726/86726 [==============================] - 7s 76us/sample - loss: 0.0780 - accuracy: 0.9933\n",
      "Epoch 67/100\n",
      "86726/86726 [==============================] - 6s 69us/sample - loss: 0.0770 - accuracy: 0.9937\n",
      "Epoch 68/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.0764 - accuracy: 0.9935\n",
      "Epoch 69/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.0766 - accuracy: 0.9935\n",
      "Epoch 70/100\n",
      "86726/86726 [==============================] - 6s 66us/sample - loss: 0.0762 - accuracy: 0.9935\n",
      "Epoch 71/100\n",
      "86726/86726 [==============================] - 5s 62us/sample - loss: 0.0757 - accuracy: 0.9936\n",
      "Epoch 72/100\n",
      "86726/86726 [==============================] - 5s 62us/sample - loss: 0.0755 - accuracy: 0.9934\n",
      "Epoch 73/100\n",
      "86726/86726 [==============================] - 5s 63us/sample - loss: 0.0755 - accuracy: 0.9935\n",
      "Epoch 74/100\n",
      "86726/86726 [==============================] - 5s 62us/sample - loss: 0.0757 - accuracy: 0.9931\n",
      "Epoch 75/100\n",
      "86726/86726 [==============================] - 5s 63us/sample - loss: 0.0746 - accuracy: 0.9936\n",
      "Epoch 76/100\n",
      "86726/86726 [==============================] - 5s 63us/sample - loss: 0.0750 - accuracy: 0.9935\n",
      "Epoch 77/100\n",
      "86726/86726 [==============================] - 6s 68us/sample - loss: 0.0750 - accuracy: 0.9934\n",
      "Epoch 78/100\n",
      "86726/86726 [==============================] - 6s 72us/sample - loss: 0.0741 - accuracy: 0.9937\n",
      "Epoch 79/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.0741 - accuracy: 0.9936\n",
      "Epoch 80/100\n",
      "86726/86726 [==============================] - 5s 61us/sample - loss: 0.0719 - accuracy: 0.9942\n",
      "Epoch 81/100\n",
      "86726/86726 [==============================] - 6s 70us/sample - loss: 0.0732 - accuracy: 0.9935\n",
      "Epoch 82/100\n",
      "86726/86726 [==============================] - 7s 78us/sample - loss: 0.0729 - accuracy: 0.9935\n",
      "Epoch 83/100\n",
      "86726/86726 [==============================] - 6s 66us/sample - loss: 0.0720 - accuracy: 0.9940\n",
      "Epoch 84/100\n",
      "86726/86726 [==============================] - 5s 63us/sample - loss: 0.0706 - accuracy: 0.9942\n",
      "Epoch 85/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.0708 - accuracy: 0.9943\n",
      "Epoch 86/100\n",
      "86726/86726 [==============================] - 7s 75us/sample - loss: 0.0720 - accuracy: 0.9932\n",
      "Epoch 87/100\n",
      "86726/86726 [==============================] - 6s 64us/sample - loss: 0.0718 - accuracy: 0.9938\n",
      "Epoch 88/100\n",
      "86726/86726 [==============================] - 6s 65us/sample - loss: 0.0722 - accuracy: 0.9934\n",
      "Epoch 89/100\n",
      "86726/86726 [==============================] - 7s 83us/sample - loss: 0.0712 - accuracy: 0.9939\n",
      "Epoch 90/100\n",
      "86726/86726 [==============================] - 7s 80us/sample - loss: 0.0716 - accuracy: 0.9937\n",
      "Epoch 91/100\n",
      "86726/86726 [==============================] - 6s 71us/sample - loss: 0.0726 - accuracy: 0.9935\n",
      "Epoch 92/100\n",
      "86726/86726 [==============================] - 7s 77us/sample - loss: 0.0708 - accuracy: 0.9938\n",
      "Epoch 93/100\n",
      "86726/86726 [==============================] - 6s 71us/sample - loss: 0.0700 - accuracy: 0.9941\n",
      "Epoch 94/100\n",
      "86726/86726 [==============================] - 7s 77us/sample - loss: 0.0713 - accuracy: 0.9936\n",
      "Epoch 95/100\n",
      "86726/86726 [==============================] - 6s 70us/sample - loss: 0.0700 - accuracy: 0.9942\n",
      "Epoch 96/100\n",
      "86726/86726 [==============================] - 6s 73us/sample - loss: 0.0701 - accuracy: 0.9940\n",
      "Epoch 97/100\n",
      "86726/86726 [==============================] - 6s 75us/sample - loss: 0.0699 - accuracy: 0.9939\n",
      "Epoch 98/100\n",
      "86726/86726 [==============================] - 6s 70us/sample - loss: 0.0704 - accuracy: 0.9937\n",
      "Epoch 99/100\n",
      "86726/86726 [==============================] - 6s 73us/sample - loss: 0.0700 - accuracy: 0.9936\n",
      "Epoch 100/100\n",
      "86726/86726 [==============================] - 7s 76us/sample - loss: 0.0710 - accuracy: 0.9932\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, batch_size=512, epochs= 100, use_multiprocessing=True,verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model,binarizer,dataset,triples):\n",
    "    x_=[]\n",
    "    y_=[]\n",
    "    \n",
    "    hits = []\n",
    "    ranks=[]\n",
    "    for i in range(10):\n",
    "        hits.append([])\n",
    "\n",
    "    rank_per_relation=dict()\n",
    "        \n",
    "    for i in triples:\n",
    "        s,p,o=i\n",
    "        x_.append((entitiy_idx[s],entitiy_idx[o]))\n",
    "        y_.append(p)\n",
    "    \n",
    "    \n",
    "    tensor_pred=torch.from_numpy(model.predict(np.array(x_)))\n",
    "    \n",
    "    _, ranked_predictions =tensor_pred.topk(k=len(binarizer.classes_))\n",
    "    \n",
    "    ranked_predictions=ranked_predictions.numpy()\n",
    "    \n",
    "    \n",
    "    assert len(ranked_predictions)==len(y_)\n",
    "    \n",
    "    classes_=binarizer.classes_.tolist()\n",
    "        \n",
    "    for i in range(len(y_)):\n",
    "        true_relation=y_[i]\n",
    "        ith_class=classes_.index(true_relation)\n",
    "        \n",
    "        rank = np.where(ranked_predictions[i]==ith_class)[0]\n",
    "        \n",
    "\n",
    "        rank_per_relation.setdefault(true_relation, []).append(rank+1)\n",
    "        \n",
    "        ranks.append(rank+1)\n",
    "        \n",
    "        for hits_level in range(10):\n",
    "            if rank <= hits_level:\n",
    "                hits[hits_level].append(1.0)\n",
    "    \n",
    "\n",
    "    hits=np.array(hits)\n",
    "    ranks=np.array(ranks)\n",
    "    print('########## Relation Prediction Results ##########')\n",
    "\n",
    "    print('Mean Hits @5: {0}'.format(sum(hits[4]) / (float(len(y_)))))\n",
    "    print('Mean Hits @3: {0}'.format(sum(hits[2]) / (float(len(y_)))))\n",
    "    print('Mean @1: {0}'.format(sum(hits[0]) / (float(len(y_)))))\n",
    "    print('Mean rank: {0}'.format(np.mean(ranks)))\n",
    "    print('Mean reciprocal rank: {0}'.format(np.mean(1. / ranks)))\n",
    "    \n",
    "    print('########## Relation Prediction Analysis ##########')\n",
    "    \n",
    "    for pred, ranks in rank_per_relation.items():\n",
    "        ranks=np.array(ranks)\n",
    "        \n",
    "        average_hit_at_1=np.sum(ranks==1)/len(ranks)\n",
    "        average_hit_at_3=np.sum(ranks<=3)/len(ranks)\n",
    "        average_hit_at_5=np.sum(ranks<=5)/len(ranks)\n",
    "        \n",
    "        print('{0}:\\t Hits@1:\\t{1:.3f}'.format(pred,average_hit_at_1))\n",
    "        print('{0}:\\t Hits@3:\\t{1:.3f}'.format(pred,average_hit_at_3))\n",
    "        print('{0}:\\t Hits@5:\\t{1:.3f}'.format(pred,average_hit_at_5))\n",
    "        print('{0}:\\t MRR:\\t{1:.3f}\\t number of occurrence {2}'.format(pred,np.mean(1. / ranks),len(ranks)))\n",
    "        print('################################')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Relation Prediction Results ##########\n",
      "Mean Hits @5: 1.0\n",
      "Mean Hits @3: 1.0\n",
      "Mean @1: 0.9984913917199286\n",
      "Mean rank: 1.0015086082800715\n",
      "Mean reciprocal rank: 0.9992456958599643\n",
      "########## Relation Prediction Analysis ##########\n",
      "_hypernym:\t Hits@1:\t0.999\n",
      "_hypernym:\t Hits@3:\t1.000\n",
      "_hypernym:\t Hits@5:\t1.000\n",
      "_hypernym:\t MRR:\t1.000\t number of occurrence 34796\n",
      "################################\n",
      "_derivationally_related_form:\t Hits@1:\t0.999\n",
      "_derivationally_related_form:\t Hits@3:\t1.000\n",
      "_derivationally_related_form:\t Hits@5:\t1.000\n",
      "_derivationally_related_form:\t MRR:\t0.999\t number of occurrence 29715\n",
      "################################\n",
      "_instance_hypernym:\t Hits@1:\t1.000\n",
      "_instance_hypernym:\t Hits@3:\t1.000\n",
      "_instance_hypernym:\t Hits@5:\t1.000\n",
      "_instance_hypernym:\t MRR:\t1.000\t number of occurrence 2921\n",
      "################################\n",
      "_also_see:\t Hits@1:\t0.995\n",
      "_also_see:\t Hits@3:\t1.000\n",
      "_also_see:\t Hits@5:\t1.000\n",
      "_also_see:\t MRR:\t0.998\t number of occurrence 1299\n",
      "################################\n",
      "_member_meronym:\t Hits@1:\t0.999\n",
      "_member_meronym:\t Hits@3:\t1.000\n",
      "_member_meronym:\t Hits@5:\t1.000\n",
      "_member_meronym:\t MRR:\t1.000\t number of occurrence 7402\n",
      "################################\n",
      "_synset_domain_topic_of:\t Hits@1:\t0.991\n",
      "_synset_domain_topic_of:\t Hits@3:\t1.000\n",
      "_synset_domain_topic_of:\t Hits@5:\t1.000\n",
      "_synset_domain_topic_of:\t MRR:\t0.996\t number of occurrence 3116\n",
      "################################\n",
      "_has_part:\t Hits@1:\t0.999\n",
      "_has_part:\t Hits@3:\t1.000\n",
      "_has_part:\t Hits@5:\t1.000\n",
      "_has_part:\t MRR:\t0.999\t number of occurrence 4816\n",
      "################################\n",
      "_member_of_domain_usage:\t Hits@1:\t1.000\n",
      "_member_of_domain_usage:\t Hits@3:\t1.000\n",
      "_member_of_domain_usage:\t Hits@5:\t1.000\n",
      "_member_of_domain_usage:\t MRR:\t1.000\t number of occurrence 629\n",
      "################################\n",
      "_member_of_domain_region:\t Hits@1:\t0.997\n",
      "_member_of_domain_region:\t Hits@3:\t1.000\n",
      "_member_of_domain_region:\t Hits@5:\t1.000\n",
      "_member_of_domain_region:\t MRR:\t0.998\t number of occurrence 923\n",
      "################################\n",
      "_verb_group:\t Hits@1:\t0.988\n",
      "_verb_group:\t Hits@3:\t1.000\n",
      "_verb_group:\t Hits@5:\t1.000\n",
      "_verb_group:\t MRR:\t0.994\t number of occurrence 1138\n",
      "################################\n",
      "_similar_to:\t Hits@1:\t1.000\n",
      "_similar_to:\t Hits@3:\t1.000\n",
      "_similar_to:\t Hits@5:\t1.000\n",
      "_similar_to:\t MRR:\t1.000\t number of occurrence 80\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "evaluation(model,binarizer,dataset,dataset.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Relation Prediction Results ##########\n",
      "Mean Hits @5: 0.9946883852691218\n",
      "Mean Hits @3: 0.9812322946175638\n",
      "Mean @1: 0.8753541076487252\n",
      "Mean rank: 1.1993626062322946\n",
      "Mean reciprocal rank: 0.9295466297945053\n",
      "########## Relation Prediction Analysis ##########\n",
      "_hypernym:\t Hits@1:\t0.828\n",
      "_hypernym:\t Hits@3:\t0.979\n",
      "_hypernym:\t Hits@5:\t0.996\n",
      "_hypernym:\t MRR:\t0.906\t number of occurrence 1010\n",
      "################################\n",
      "_derivationally_related_form:\t Hits@1:\t0.950\n",
      "_derivationally_related_form:\t Hits@3:\t0.997\n",
      "_derivationally_related_form:\t Hits@5:\t0.998\n",
      "_derivationally_related_form:\t MRR:\t0.973\t number of occurrence 1076\n",
      "################################\n",
      "_synset_domain_topic_of:\t Hits@1:\t0.990\n",
      "_synset_domain_topic_of:\t Hits@3:\t1.000\n",
      "_synset_domain_topic_of:\t Hits@5:\t1.000\n",
      "_synset_domain_topic_of:\t MRR:\t0.995\t number of occurrence 102\n",
      "################################\n",
      "_member_meronym:\t Hits@1:\t0.803\n",
      "_member_meronym:\t Hits@3:\t0.962\n",
      "_member_meronym:\t Hits@5:\t0.989\n",
      "_member_meronym:\t MRR:\t0.881\t number of occurrence 264\n",
      "################################\n",
      "_instance_hypernym:\t Hits@1:\t0.929\n",
      "_instance_hypernym:\t Hits@3:\t0.941\n",
      "_instance_hypernym:\t Hits@5:\t0.965\n",
      "_instance_hypernym:\t MRR:\t0.943\t number of occurrence 85\n",
      "################################\n",
      "_has_part:\t Hits@1:\t0.753\n",
      "_has_part:\t Hits@3:\t0.952\n",
      "_has_part:\t Hits@5:\t0.986\n",
      "_has_part:\t MRR:\t0.856\t number of occurrence 146\n",
      "################################\n",
      "_verb_group:\t Hits@1:\t0.837\n",
      "_verb_group:\t Hits@3:\t1.000\n",
      "_verb_group:\t Hits@5:\t1.000\n",
      "_verb_group:\t MRR:\t0.915\t number of occurrence 43\n",
      "################################\n",
      "_also_see:\t Hits@1:\t0.756\n",
      "_also_see:\t Hits@3:\t0.854\n",
      "_also_see:\t Hits@5:\t1.000\n",
      "_also_see:\t MRR:\t0.830\t number of occurrence 41\n",
      "################################\n",
      "_member_of_domain_usage:\t Hits@1:\t1.000\n",
      "_member_of_domain_usage:\t Hits@3:\t1.000\n",
      "_member_of_domain_usage:\t Hits@5:\t1.000\n",
      "_member_of_domain_usage:\t MRR:\t1.000\t number of occurrence 22\n",
      "################################\n",
      "_member_of_domain_region:\t Hits@1:\t0.625\n",
      "_member_of_domain_region:\t Hits@3:\t0.969\n",
      "_member_of_domain_region:\t Hits@5:\t0.969\n",
      "_member_of_domain_region:\t MRR:\t0.802\t number of occurrence 32\n",
      "################################\n",
      "_similar_to:\t Hits@1:\t1.000\n",
      "_similar_to:\t Hits@3:\t1.000\n",
      "_similar_to:\t Hits@5:\t1.000\n",
      "_similar_to:\t MRR:\t1.000\t number of occurrence 3\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "evaluation(model,binarizer,dataset,dataset.valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Relation Prediction Results ##########\n",
      "Mean Hits @5: 0.9952120383036935\n",
      "Mean Hits @3: 0.9829001367989056\n",
      "Mean @1: 0.874829001367989\n",
      "Mean rank: 1.1932284541723666\n",
      "Mean reciprocal rank: 0.9295151781642889\n",
      "########## Relation Prediction Analysis ##########\n",
      "_member_of_domain_usage:\t Hits@1:\t0.955\n",
      "_member_of_domain_usage:\t Hits@3:\t1.000\n",
      "_member_of_domain_usage:\t Hits@5:\t1.000\n",
      "_member_of_domain_usage:\t MRR:\t0.977\t number of occurrence 22\n",
      "################################\n",
      "_verb_group:\t Hits@1:\t0.744\n",
      "_verb_group:\t Hits@3:\t1.000\n",
      "_verb_group:\t Hits@5:\t1.000\n",
      "_verb_group:\t MRR:\t0.868\t number of occurrence 39\n",
      "################################\n",
      "_member_of_domain_region:\t Hits@1:\t0.885\n",
      "_member_of_domain_region:\t Hits@3:\t0.962\n",
      "_member_of_domain_region:\t Hits@5:\t0.962\n",
      "_member_of_domain_region:\t MRR:\t0.927\t number of occurrence 26\n",
      "################################\n",
      "_member_meronym:\t Hits@1:\t0.785\n",
      "_member_meronym:\t Hits@3:\t0.947\n",
      "_member_meronym:\t Hits@5:\t0.992\n",
      "_member_meronym:\t MRR:\t0.872\t number of occurrence 246\n",
      "################################\n",
      "_hypernym:\t Hits@1:\t0.841\n",
      "_hypernym:\t Hits@3:\t0.979\n",
      "_hypernym:\t Hits@5:\t0.995\n",
      "_hypernym:\t MRR:\t0.912\t number of occurrence 1067\n",
      "################################\n",
      "_has_part:\t Hits@1:\t0.651\n",
      "_has_part:\t Hits@3:\t0.964\n",
      "_has_part:\t Hits@5:\t0.982\n",
      "_has_part:\t MRR:\t0.796\t number of occurrence 169\n",
      "################################\n",
      "_derivationally_related_form:\t Hits@1:\t0.963\n",
      "_derivationally_related_form:\t Hits@3:\t0.999\n",
      "_derivationally_related_form:\t Hits@5:\t0.999\n",
      "_derivationally_related_form:\t MRR:\t0.981\t number of occurrence 1074\n",
      "################################\n",
      "_also_see:\t Hits@1:\t0.821\n",
      "_also_see:\t Hits@3:\t0.982\n",
      "_also_see:\t Hits@5:\t1.000\n",
      "_also_see:\t MRR:\t0.894\t number of occurrence 56\n",
      "################################\n",
      "_instance_hypernym:\t Hits@1:\t0.911\n",
      "_instance_hypernym:\t Hits@3:\t0.973\n",
      "_instance_hypernym:\t Hits@5:\t0.982\n",
      "_instance_hypernym:\t MRR:\t0.946\t number of occurrence 112\n",
      "################################\n",
      "_synset_domain_topic_of:\t Hits@1:\t0.909\n",
      "_synset_domain_topic_of:\t Hits@3:\t0.973\n",
      "_synset_domain_topic_of:\t Hits@5:\t1.000\n",
      "_synset_domain_topic_of:\t MRR:\t0.946\t number of occurrence 110\n",
      "################################\n",
      "_similar_to:\t Hits@1:\t1.000\n",
      "_similar_to:\t Hits@3:\t1.000\n",
      "_similar_to:\t Hits@5:\t1.000\n",
      "_similar_to:\t MRR:\t1.000\t number of occurrence 3\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "evaluation(model,binarizer,dataset,dataset.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shallom)",
   "language": "python",
   "name": "shallom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
